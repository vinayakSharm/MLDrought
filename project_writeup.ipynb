{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSCI 4622 Final Project Write-Up\n",
    "\n",
    "Myeongseon Lee, Vinayak Sharma, Jaskrit Singh, Joshua Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project topic\n",
    "\n",
    "* Is there a clear explanation of what this project is about? Does it state clearly which type of problem (e.g. type of learning and type of task)?\n",
    "* Does it state the motivation or the goal (or why itâ€™s important, what goal the team wants to achieve, or want to learn) clearly?\n",
    "* (Extra credit) Is the project topic creative? Requires collecting data (e.g. scraping)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: Our project is motivated by a project found on Kaggle. The dataset consists of weather data, with the overall goal of predicting drought. Drought is measured continuously in our data, but technically can be a categorical variable. We'll treat it as both in this project. Our goals are to implement the knowledge we've gained from this class in order to model our data. \n",
    "\n",
    "This project is supervised learning, for classification (and regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data\n",
    "\n",
    "* Is the data source properly quoted and described? (including links, brief explanations)\n",
    "* Do they explain the data description properly? The data description can include the data size\n",
    "    * e.g. for tabulated data: number of samples/rows, number of features/columns, byte size if a huge file, data type of each feature (or just a summary if too many features e.g. 10 categorical, 20 numeric features), description of features (at least some key features if too many), whether the data is a multi-table form or gathered from multiple data source.\n",
    "    * e.g. for images: you can include how many samples, number of channels (color or gray or more?) or modalities, image file format, whether images have the same dimension or not, etc.\n",
    "    * e.g. sequential data: texts, sound file; please describe appropriate properties such as how many documents or words, how many sound files with typical length (are they the same or variable), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data cleaning\n",
    "\n",
    "Example score breakdown for tabulated data format: no cleaning 0 pts (if the data was given perfectly cleaned, just give a default score of 5 pts), data types munging +1, drop NA +1, impute +1, identify imbalance +1, and identify data-specific potential problem +1.\n",
    "* Does it include clear explanations on how and why cleaning is performed?\n",
    "    * (e.g.) the author decided to drop a feature because it had too many NaN values and the data cannot be imputed.\n",
    "    * (e.g.) the author decided to impute certain values in a feature because the number of missing values was small and he/she was able to find similar samples OR, he/she used an average value or interpolated value, etc.\n",
    "    * (e.g.) the author removed some features because there are too many of them and they are not relevant to the problem, or he/she knows only a few certain features are important based on their domain knowledge judgment.\n",
    "    * (e.g.) the author removed a certain sample (row) or a value because it is an outlier.\n",
    "    * (e.g.) if the project is on text data, is stopword filtering conducted? If no, why not?\n",
    "* Does it have conclusions or discussions? E.g. the data cleaning summary, findings, discussing foreseen difficulties, and/or analysis strategy.\n",
    "* Does it have a proper visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to whatever the path is \n",
    "path = '../archive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+'train_timeseries.csv')\n",
    "valid = pd.read_csv(path+'validation_timeseries.csv')\n",
    "test = pd.read_csv(path+'test_timeseries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we will interpolate the drought data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data preprocessing\n",
    "train['date'] = pd.to_datetime(train['date']) # parse date.\n",
    "train['score'] = train['score'].apply(pd.to_numeric).interpolate()\n",
    "#train['drought_level'] = np.floor(train['score']) # classify drought level.\n",
    "#train.drop(columns=['fips', 'score'], inplace=True) # remove un-necessary columns.\n",
    "train.dropna(inplace=True) # remove all examples containing NaN.\n",
    "\n",
    "# validation data preprocessing\n",
    "valid['date'] = pd.to_datetime(valid['date']) # parse date.\n",
    "valid['score'] = valid['score'].apply(pd.to_numeric).interpolate()\n",
    "#valid['drought_level'] = np.floor(valid['score']) # classify drought level.\n",
    "#valid.drop(columns=['fips', 'score'], inplace=True) # remove un-necessary columns.\n",
    "valid.dropna(inplace=True) # remove all examples containing NaN.\n",
    "\n",
    "# testing data preprocessing\n",
    "test['date'] = pd.to_datetime(test['date']) # parse date.\n",
    "test['score'] = test['score'].apply(pd.to_numeric).interpolate()\n",
    "#test['drought_level'] = np.floor(test['score']) # classify drought level.\n",
    "#test.drop(columns=['fips', 'score'], inplace=True) # remove un-necessary columns.\n",
    "test.dropna(inplace=True) # remove all examples containing NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis\n",
    "\n",
    "Example score breakdown for tabulated data format: no cleaning 0 pts (if the data was given perfectly cleaned, just give a default score of 5 pts), data types munging +1, drop NA +1, impute +1, identify imbalance +1, and identify data-specific potential problem +1.\n",
    "\n",
    "* Does it include clear explanations on how and why cleaning is performed?\n",
    " \n",
    "    * (e.g.) the author decided to drop a feature because it had too many NaN values and the data cannot be imputed.\n",
    "    * (e.g.) the author decided to impute certain values in a feature because the number of missing values was small and he/she was able to find similar samples OR, he/she used an average value or interpolated value, etc.\n",
    "    * (e.g.) the author removed some features because there are too many of them and they are not relevant to the problem, or he/she knows only a few certain features are important based on their domain knowledge judgment.\n",
    "    * (e.g.) the author removed a certain sample (row) or a value because it is an outlier.\n",
    "    * (e.g.) if the project is on text data, is stopword filtering conducted? If no, why not?\n",
    "* Does it have conclusions or discussions? E.g. the data cleaning summary, findings, discussing foreseen difficulties, and/or analysis strategy.\n",
    "* Does it have a proper visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Models\n",
    "\n",
    "Example score breakdown for typical supervised learning: 8 if a proper single model is used, +2 if addresses multilinear regression/collinearity for regression models, +2 feature engineering, +2 multiple ML models, +2 hyperparam tuning, +2 regularization, or other training techniques such as cross-validation, oversampling/undersampling or similar for managing data imbalance, +2 using models not covered from class.\n",
    "* Is the choice of model(s) appropriate with the problem?\n",
    "* Is the author aware of whether interaction/collinearity between features can be a\n",
    "problem for the choice of the model and properly treat if there is interaction or collinearity (e.g. linear regression)? Or confirms that there is no such effect with the choice of the model?\n",
    "* Did the author use multiple (appropriate) models?\n",
    "* Did the author investigate which ones are important features by looking at feature\n",
    "rankings or importance from the model? (Not by a judgment which we already covered in the EDA category) Did the author use techniques to reduce overfitting or data imbalance?\n",
    "* Did the author use new techniques/models we didn't cover in the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Results and Analysis\n",
    "\n",
    "\n",
    "Example score breakdown: showing basic result 12, with a good amount of visualization +2, try different evaluation metrics +2, iterate training/evaluating and improve performance +2, show/discuss model comparison +2\n",
    "* Does it have a summary of results and analysis?\n",
    "* Does it have a proper visualization? (e.g. tables, graphs/plots, heat maps,\n",
    "statistics summary with interpretation, etc)\n",
    "* Does it use different kinds of evaluation metrics properly? (e.g. if your data is\n",
    "imbalanced, there are other metrics F1, ROC, or AUC better than mere\n",
    "accuracy). Also, does it explain why they chose the metric?\n",
    "* Does it iterate the training and evaluation process and improve the performance?\n",
    "Does it address selecting features through the iteration process?\n",
    "* Did the author compare the results from the multiple models and did an\n",
    "appropriate comparison?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Discussion and Conclusion\n",
    "\n",
    "Example score breakdown: basic reiteration of the result 6, discussion on what are the learnings and takeaways, discussion on why something didn't work +2, suggesting ways to improve +2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
