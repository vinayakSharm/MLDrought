{"cells":[{"metadata":{},"cell_type":"markdown","source":"> ## US Drought & Meteorological Data Starter Notebook\nThis notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors."},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data\nIn this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes."},{"metadata":{},"cell_type":"markdown","source":"We load the json files for training, validation and testing into the ``files`` dictionary."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nsns.set_style('white')\n\nfiles = {}\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if 'train' in filename:\n            files['train'] = os.path.join(dirname, filename)\n        if 'valid' in filename:\n            files['valid'] = os.path.join(dirname, filename)\n        if 'test' in filename:\n            files['test'] = os.path.join(dirname, filename)","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\nThis could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class2id = {\n    'None': 0,\n    'D0': 1,\n    'D1': 2,\n    'D2': 3,\n    'D3': 4,\n    'D4': 5,\n}\nid2class = {v: k for k, v in class2id.items()}","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also create a dictionary for the meteorological attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dict = json.load(open(files['valid'], 'r'))\n_, first = next(iter(valid_dict['root'].items()))\nattributes = sorted(first['values'].keys())\n\nid2attr = {i: k for i, k in enumerate(attributes)}\nattr2id = {v: k for k, v in id2attr.items()}\n\ndel valid_dict\nattr2id","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"{'PRECTOT': 0,\n 'PS': 1,\n 'QV2M': 2,\n 'T2M': 3,\n 'T2MDEW': 4,\n 'T2MWET': 5,\n 'T2M_MAX': 6,\n 'T2M_MIN': 7,\n 'T2M_RANGE': 8,\n 'TS': 9,\n 'WS10M': 10,\n 'WS10M_MAX': 11,\n 'WS10M_MIN': 12,\n 'WS10M_RANGE': 13,\n 'WS50M': 14,\n 'WS50M_MAX': 15,\n 'WS50M_MIN': 16,\n 'WS50M_RANGE': 17}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load one of 'train', 'valid' or 'test'\ndef loadXY(dataset, shuffle=True, random_state=None):\n    data_dict = json.load(open(files[dataset], 'r'))\n    keys = sorted(list(data_dict['root'].keys()))\n    \n    if shuffle:\n        if random_state is not None:\n            np.random.seed(random_state)\n        np.random.shuffle(keys)\n        \n    # float16 should be enough and saves some memory\n    X = np.zeros([len(keys), 90, 18], dtype=np.float16)\n    y = np.zeros([len(keys)], dtype=np.float16)\n    # track how many samples are skipped\n    skip_count = 0\n    for i, key in tqdm(enumerate(keys), total=len(keys), desc=f'loading {dataset} dataset'):\n        sample = data_dict['root'][key]\n        input_arr = np.zeros([90, 18])\n        try:\n            for a, j in attr2id.items():\n                input_arr[:,j] = sample['values'][a]\n            X[i-skip_count] = input_arr\n            y[i-skip_count] = float(class2id[sample['class']])\n        except:\n            skip_count += 1\n    print(f'[{dataset}]: skipped {skip_count} samples ({round(skip_count/len(keys)*100, 3)}%), loaded {len(keys)-skip_count} samples')\n    del data_dict\n    return X, y","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now load the datasets, this will take a few minutes and use ~8GB of RAM."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_train, y_train = loadXY('train', random_state=42)\nX_valid, y_valid = loadXY('valid', random_state=42)","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"loading train dataset:   0%|          | 0/118057 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62e93c6e4b14314beafeba1ae1334af"}},"metadata":{}},{"output_type":"stream","text":"[train]: skipped 33 samples (0.028%), loaded 118024 samples\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"loading valid dataset:   0%|          | 0/20729 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5353e6009145e082aca6cfed3a525f"}},"metadata":{}},{"output_type":"stream","text":"[valid]: skipped 8 samples (0.039%), loaded 20721 samples\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## LSTM\nLet's train a simple LSTM on the data, treating this as a regression problem."},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing and Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size_factor = 3\nbatch_size = 256 * batch_size_factor","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nscaler_dict = {}\n\nfor attr_id in id2attr.keys():\n    scaler_dict[attr_id] = RobustScaler().fit(\n        X_train[:,:,attr_id].reshape(-1, 1)\n    )\n    X_train[:,:,attr_id] = scaler_dict[attr_id].transform(X_train[:,:,attr_id].reshape(-1, 1)).reshape(-1, 90)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for attr_id in id2attr.keys():\n    X_valid[:,:,attr_id] = scaler_dict[attr_id].transform(X_valid[:,:,attr_id].reshape(-1, 1)).reshape(-1, 90)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\ntrain_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_data = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\nvalid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper parameters\nlr = 7e-5 * batch_size_factor\noutput_size = 1\nhidden_dim = 512\ndropout = 0.1\nn_layers = 4\nepochs = 10\nclip = 5","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass DroughtNetLSTM(nn.Module):\n    def __init__(self, output_size, num_input_features, hidden_dim, n_layers, drop_prob=0.2):\n        super(DroughtNetLSTM, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(num_input_features, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.cuda().to(dtype=torch.float32)\n        lstm_out, hidden = self.lstm(x, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (\n            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n        )\n        return hidden","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print('using GPU')\nelse:\n    device = torch.device(\"cpu\")\n    print('using CPU')\n\n\nmodel = DroughtNetLSTM(output_size, len(id2attr), hidden_dim, n_layers, dropout)\nmodel.to(device)","execution_count":31,"outputs":[{"output_type":"stream","text":"using GPU\n","name":"stdout"},{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"DroughtNetLSTM(\n  (lstm): LSTM(18, 512, num_layers=4, batch_first=True, dropout=0.1)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_function = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ncounter = 0\nvalid_loss_min = np.Inf\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nmodel.train()\n\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n    \n    for k, (inputs, labels) in tqdm(enumerate(train_loader), desc=f'epoch {i+1}/{epochs}', total=len(train_loader)):\n        counter += 1\n        h = tuple([e.data for e in h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = loss_function(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        scheduler.step()\n        \n        if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n            val_h = model.init_hidden(batch_size)\n            val_losses = []\n            model.eval()\n            labels = []\n            preds = []\n            for inp, lab in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                inp, lab = inp.to(device), lab.to(device)\n                out, val_h = model(inp, val_h)\n                val_loss = loss_function(out.squeeze(), lab.float())\n                val_losses.append(val_loss.item())\n                for l in lab:\n                    labels.append(int(l))\n                for p in out.round():\n                    if p > 5:\n                        p = 5\n                    if p < 0:\n                        p = 0\n                    preds.append(int(p))\n            \n            # log data\n            log_dict = {\n                'loss': float(loss),\n                'epoch': counter/len(train_loader),\n                'step': counter,\n                'lr': scheduler.get_last_lr()[0]\n            }\n            log_dict['validation_loss'] = np.mean(val_losses)\n            log_dict[f'macro_f1'] = f1_score(labels, preds, average='macro')\n            log_dict[f'micro_f1'] = f1_score(labels, preds, average='micro')\n            for j, f1 in enumerate(f1_score(labels, preds, average=None)):\n                log_dict[f'{id2class[j]}_f1'] = f1\n            print(log_dict)\n            \n            model.train()\n            \n            if np.mean(val_losses) <= valid_loss_min:\n                torch.save(model.state_dict(), './state_dict.pt')\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                valid_loss_min = np.mean(val_losses)","execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"epoch 1/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695bb95aa8894b8b95e98ff7e9929b7b"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 1.4948973655700684, 'epoch': 0.5032679738562091, 'step': 77, 'lr': 2.2136026153986765e-05, 'validation_loss': 1.9406463549687312, 'macro_f1': 0.13311073007851276, 'micro_f1': 0.6648137019230769, 'None_f1': 0.7986643804710766, 'D0_f1': 0.0, 'D1_f1': 0.0, 'D2_f1': 0.0, 'D3_f1': 0.0, 'D4_f1': 0.0}\nValidation loss decreased (inf --> 1.940646).  Saving model ...\n{'loss': 1.2560970783233643, 'epoch': 1.0, 'step': 153, 'lr': 5.899972870408802e-05, 'validation_loss': 1.4414000786267793, 'macro_f1': 0.15413493156166938, 'micro_f1': 0.425380608974359, 'None_f1': 0.6179924242424242, 'D0_f1': 0.28008118295158013, 'D1_f1': 0.026735982176011884, 'D2_f1': 0.0, 'D3_f1': 0.0, 'D4_f1': 0.0}\nValidation loss decreased (1.940646 --> 1.441400).  Saving model ...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 2/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3090abd34f452fb830b2948e2c22e1"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.9294934272766113, 'epoch': 1.5032679738562091, 'step': 230, 'lr': 0.00010989141933666985, 'validation_loss': 1.0183692574501038, 'macro_f1': 0.2079546303216824, 'micro_f1': 0.5318008814102564, 'None_f1': 0.7476325958639433, 'D0_f1': 0.2911502500543596, 'D1_f1': 0.20196413321947054, 'D2_f1': 0.006980802792321117, 'D3_f1': 0.0, 'D4_f1': 0.0}\nValidation loss decreased (1.441400 --> 1.018369).  Saving model ...\n{'loss': 1.0269651412963867, 'epoch': 2.0, 'step': 306, 'lr': 0.00015999866590906423, 'validation_loss': 1.0108393453634703, 'macro_f1': 0.21708474616889575, 'micro_f1': 0.5995092147435898, 'None_f1': 0.8130984749970446, 'D0_f1': 0.3260082105771553, 'D1_f1': 0.15592515592515593, 'D2_f1': 0.007476635514018692, 'D3_f1': 0.0, 'D4_f1': 0.0}\nValidation loss decreased (1.018369 --> 1.010839).  Saving model ...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 3/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796d9c102728492d91024564df7bb2cd"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.8433843851089478, 'epoch': 2.503267973856209, 'step': 383, 'lr': 0.00019695265317696693, 'validation_loss': 0.8858677882414597, 'macro_f1': 0.2515007184831327, 'micro_f1': 0.5903445512820513, 'None_f1': 0.8020627694291124, 'D0_f1': 0.3179069155929529, 'D1_f1': 0.22693096377306904, 'D2_f1': 0.13636363636363635, 'D3_f1': 0.025740025740025742, 'D4_f1': 0.0}\nValidation loss decreased (1.010839 --> 0.885868).  Saving model ...\n{'loss': 0.9715746641159058, 'epoch': 3.0, 'step': 459, 'lr': 0.00020999954827091016, 'validation_loss': 0.8878425038777865, 'macro_f1': 0.24606001656070434, 'micro_f1': 0.5919971955128205, 'None_f1': 0.8058637401369809, 'D0_f1': 0.31373971940009676, 'D1_f1': 0.18346034434095398, 'D2_f1': 0.1478510028653295, 'D3_f1': 0.02544529262086514, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 4/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a30a482300d540d2b7fb575f14ab4eb1"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.813015341758728, 'epoch': 3.503267973856209, 'step': 536, 'lr': 0.0002072636467146789, 'validation_loss': 0.8319744880382831, 'macro_f1': 0.2892818431135446, 'micro_f1': 0.5827824519230769, 'None_f1': 0.7917633410672854, 'D0_f1': 0.32350718065003775, 'D1_f1': 0.23516336963769732, 'D2_f1': 0.20761245674740486, 'D3_f1': 0.17764471057884232, 'D4_f1': 0.0}\nValidation loss decreased (0.885868 --> 0.831974).  Saving model ...\n{'loss': 0.9177225828170776, 'epoch': 4.0, 'step': 612, 'lr': 0.00019946773057601897, 'validation_loss': 0.8629364944421328, 'macro_f1': 0.26560898519169046, 'micro_f1': 0.590645032051282, 'None_f1': 0.8005311657478572, 'D0_f1': 0.32106289614822037, 'D1_f1': 0.1862125789618237, 'D2_f1': 0.15476904619076182, 'D3_f1': 0.13107822410147993, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 5/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b709467758043018866d842bc987ea6"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.7793967723846436, 'epoch': 4.503267973856209, 'step': 689, 'lr': 0.00018680355311135406, 'validation_loss': 0.7511413785127493, 'macro_f1': 0.29430302920459234, 'micro_f1': 0.6131310096153846, 'None_f1': 0.817050505050505, 'D0_f1': 0.33986086060293735, 'D1_f1': 0.27027027027027023, 'D2_f1': 0.2440419447092469, 'D3_f1': 0.0945945945945946, 'D4_f1': 0.0}\nValidation loss decreased (0.831974 --> 0.751141).  Saving model ...\n{'loss': 0.8934049606323242, 'epoch': 5.0, 'step': 765, 'lr': 0.00017022550345601386, 'validation_loss': 0.8065212758687826, 'macro_f1': 0.3044465441152184, 'micro_f1': 0.5799278846153846, 'None_f1': 0.7890188103711235, 'D0_f1': 0.3379515091538842, 'D1_f1': 0.242544731610338, 'D2_f1': 0.22645617855198694, 'D3_f1': 0.2307080350039777, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 6/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264de84393344d59af42b72043f1e74c"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.7399887442588806, 'epoch': 5.503267973856209, 'step': 842, 'lr': 0.00015014134574573628, 'validation_loss': 0.7645601997008691, 'macro_f1': 0.29321824625648735, 'micro_f1': 0.6049679487179487, 'None_f1': 0.8110345958353828, 'D0_f1': 0.34185869840465943, 'D1_f1': 0.274831243972999, 'D2_f1': 0.22417355371900827, 'D3_f1': 0.10741138560687434, 'D4_f1': 0.0}\n{'loss': 0.864919900894165, 'epoch': 6.0, 'step': 918, 'lr': 0.00012806464862439231, 'validation_loss': 0.7647277575272781, 'macro_f1': 0.3043130254930518, 'micro_f1': 0.5795272435897436, 'None_f1': 0.786897980321077, 'D0_f1': 0.3468002225932109, 'D1_f1': 0.2755868544600939, 'D2_f1': 0.23081110506260205, 'D3_f1': 0.18578199052132705, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 7/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931fd56be0e049ee81c438174e7b5926"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.7275607585906982, 'epoch': 6.503267973856209, 'step': 995, 'lr': 0.00010453842441905225, 'validation_loss': 0.80767488021117, 'macro_f1': 0.2684237036506948, 'micro_f1': 0.5836338141025641, 'None_f1': 0.7944470075074446, 'D0_f1': 0.33937282229965154, 'D1_f1': 0.2581567039771374, 'D2_f1': 0.17669376693766936, 'D3_f1': 0.04187192118226601, 'D4_f1': 0.0}\n{'loss': 0.8345908522605896, 'epoch': 7.0, 'step': 1071, 'lr': 8.133564044950874e-05, 'validation_loss': 0.7775539526572595, 'macro_f1': 0.28053575260958213, 'micro_f1': 0.580078125, 'None_f1': 0.7881562620341448, 'D0_f1': 0.3514840913944053, 'D1_f1': 0.2684049079754601, 'D2_f1': 0.18089315997738836, 'D3_f1': 0.09427609427609428, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 8/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f6c92abbc947b3b126b03f057ddac1"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.7290555834770203, 'epoch': 7.503267973856209, 'step': 1148, 'lr': 5.9027006983178876e-05, 'validation_loss': 0.7990255493384141, 'macro_f1': 0.28880992179900933, 'micro_f1': 0.5863882211538461, 'None_f1': 0.7965077233042311, 'D0_f1': 0.34230679972151307, 'D1_f1': 0.2428953121204761, 'D2_f1': 0.1879656160458453, 'D3_f1': 0.16318407960199005, 'D4_f1': 0.0}\n{'loss': 0.7926998138427734, 'epoch': 8.0, 'step': 1224, 'lr': 3.9293732094074813e-05, 'validation_loss': 0.8101752262849075, 'macro_f1': 0.2854590612728804, 'micro_f1': 0.6034154647435898, 'None_f1': 0.8116552003237557, 'D0_f1': 0.3301044156497673, 'D1_f1': 0.24546370967741932, 'D2_f1': 0.1727962638645651, 'D3_f1': 0.15273477812177502, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 9/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db0340fb1ee4b578d8f8dfd779d3315"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.7214528918266296, 'epoch': 8.50326797385621, 'step': 1301, 'lr': 2.2621187822196478e-05, 'validation_loss': 0.7991858629079965, 'macro_f1': 0.28605553177384535, 'micro_f1': 0.5848357371794872, 'None_f1': 0.7959124087591241, 'D0_f1': 0.327277102635062, 'D1_f1': 0.2528301886792452, 'D2_f1': 0.18330414477524812, 'D3_f1': 0.15700934579439252, 'D4_f1': 0.0}\n{'loss': 0.779788613319397, 'epoch': 9.0, 'step': 1377, 'lr': 1.0265839118053297e-05, 'validation_loss': 0.7779870904408969, 'macro_f1': 0.2942362025876522, 'micro_f1': 0.5939503205128205, 'None_f1': 0.8038987971795936, 'D0_f1': 0.33398580866161004, 'D1_f1': 0.2567149988115046, 'D2_f1': 0.19329164297896534, 'D3_f1': 0.17752596789423986, 'D4_f1': 0.0}\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"epoch 10/10:   0%|          | 0/153 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88ca9f4d6304215b531ba6ab59f6037"}},"metadata":{}},{"output_type":"stream","text":"{'loss': 0.7054564356803894, 'epoch': 9.50326797385621, 'step': 1454, 'lr': 2.531585909008714e-06, 'validation_loss': 0.7585876148480636, 'macro_f1': 0.2951007256245468, 'micro_f1': 0.6006610576923077, 'None_f1': 0.8089591567852438, 'D0_f1': 0.33775075987841946, 'D1_f1': 0.2632771205638057, 'D2_f1': 0.19152640742890306, 'D3_f1': 0.16909090909090907, 'D4_f1': 0.0}\n{'loss': 0.7762449979782104, 'epoch': 10.0, 'step': 1530, 'lr': 1.2917290898069753e-09, 'validation_loss': 0.7651898906781123, 'macro_f1': 0.29370493271259696, 'micro_f1': 0.5978565705128205, 'None_f1': 0.8069972770030529, 'D0_f1': 0.33548387096774196, 'D1_f1': 0.25910329452563785, 'D2_f1': 0.189424753050552, 'D3_f1': 0.17122040072859745, 'D4_f1': 0.0}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Best Macro F1 - **0.304**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}